{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from models.prefix_gptneox_model import PrefixGPTNeoXLMHeadModel\n",
    "from utils.args_utils import Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "random_seed = 100\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ARGS\n",
    "args = Args()\n",
    "\n",
    "args.pretrained_model = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "args.special_tokens = None\n",
    "# Pretrained LM 웨이트 고정\n",
    "args.freeze_plm = True\n",
    "# Prefix 웨이트 학습\n",
    "args.freeze_prefix = False\n",
    "\n",
    "# hyperparams\n",
    "args.prefix_dropout = 0.1\n",
    "args.prefix_sequence_length = 8\n",
    "args.mid_dim = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 8.\n",
      "dict_keys(['input_tokens', 'wte.weight', 'control_trans.0.weight', 'control_trans.0.bias', 'control_trans.2.weight', 'control_trans.2.bias'])\n",
      "Trained Model\n"
     ]
    }
   ],
   "source": [
    "# Load Initial Model\n",
    "model = PrefixGPTNeoXLMHeadModel(args)\n",
    "\n",
    "processed_dict = torch.load(\"prefix_weights/gptneox_ep30_1r1e-5.bin\")\n",
    "print(processed_dict.keys())\n",
    "# strict=False 여야 부분 웨이트만 로드\n",
    "model.load_state_dict(processed_dict, strict=False)\n",
    "model.eval()\n",
    "print(\"Trained Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 8.\n",
      "BASELINE COMPARISION MODEL\n"
     ]
    }
   ],
   "source": [
    "model2 = PrefixGPTNeoXLMHeadModel(args)\n",
    "model2.eval()\n",
    "print(\"BASELINE COMPARISION MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   31,    86,    33,  1569, 29093,  9144,   829,   563,   441,    17,\n",
      "          5485,    36,    64,   224]])\n",
      "<s>새신발 샀는데 비와. [A] okay!</s>비가 오면 안되겠네요.</S>비오는 날은 조심하세요.<s>(조심해서 나쁠 건 없죠)</A>그렇긴 하죠.<\n"
     ]
    }
   ],
   "source": [
    "# generate 함수 이용\n",
    "s = \"<s>새신발 샀는데 비와.\"\n",
    "model_in = s + ' [A] '\n",
    "inputs = model.tokenizer([model_in], max_length=256, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(inputs[\"input_ids\"])\n",
    "generated_ids = model.generate(inputs[\"input_ids\"], \\\n",
    "        attention_mask = inputs[\"attention_mask\"], \\\n",
    "        num_beams=1, min_length=32, do_sample = False, \\\n",
    "        max_length=63, repetition_penalty = 1.2, no_repeat_ngram_size = 3, early_stopping = True)\n",
    "\n",
    "print(model.tokenizer.batch_decode(generated_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   31,    86,    33,  1569, 29093,  9144,   829,   563,   441,    17,\n",
      "          5485,    36,    64,   224]])\n",
      "<s>새신발 샀는데 비와. [A] o o [B] u u [C] e e e [D] d d [E] ed ed [F] f f [G] g g [H] he he [I] i\n"
     ]
    }
   ],
   "source": [
    "# generate 함수 이용\n",
    "s = \"<s>새신발 샀는데 비와.\"\n",
    "model_in = s + ' [A] '\n",
    "inputs = model.tokenizer([model_in], max_length=256, return_tensors=\"pt\", add_special_tokens=True)\n",
    "print(inputs[\"input_ids\"])\n",
    "generated_ids = model2.generate(inputs[\"input_ids\"], \\\n",
    "        attention_mask = inputs[\"attention_mask\"], \\\n",
    "        num_beams=5, min_length=32, do_sample = False, \\\n",
    "        max_length=63, repetition_penalty = 1.2, no_repeat_ngram_size = 3, early_stopping = True)\n",
    "\n",
    "print(model2.tokenizer.batch_decode(generated_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
