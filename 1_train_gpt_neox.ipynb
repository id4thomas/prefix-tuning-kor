{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yrsong/anaconda3/envs/huggingface/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "from models.prefix_gptneox_model import PrefixGPTNeoXLMHeadModel\n",
    "\n",
    "from train_utils.data_utils import batch_tokenize_preprocess_decoder\n",
    "from utils.args_utils import Args\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.24.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=100):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 100\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL ARGS\n",
    "args = Args()\n",
    "\n",
    "args.pretrained_model = \"EleutherAI/polyglot-ko-1.3b\"\n",
    "args.special_tokens = None\n",
    "# Pretrained LM 웨이트 고정\n",
    "args.freeze_plm = True\n",
    "# Prefix 웨이트 학습\n",
    "args.freeze_prefix = False\n",
    "\n",
    "# hyperparams\n",
    "args.prefix_dropout = 0.1\n",
    "args.prefix_sequence_length = 8\n",
    "args.mid_dim = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix-tuning sequence length is 8.\n"
     ]
    }
   ],
   "source": [
    "# Load Initial Model\n",
    "model = PrefixGPTNeoXLMHeadModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9576, 2) (1064, 2) Index(['source', 'target'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 24.60ba/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 43.62ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Data\n",
    "# Data from https://github.com/songys/Chatbot_datas\n",
    "tokenizer = model.tokenizer\n",
    "max_length = 128\n",
    "\n",
    "df_train = pd.read_csv(\"processed_data/train.tsv\", sep = \"\\t\")\n",
    "df_val = pd.read_csv(\"processed_data/val.tsv\", sep = \"\\t\")\n",
    "print(df_train.shape, df_val.shape, df_train.columns)\n",
    "\n",
    "tr_ds = Dataset.from_pandas(df_train)\n",
    "val_ds = Dataset.from_pandas(df_val)\n",
    "\n",
    "tr_ds = tr_ds.map(\n",
    "    lambda batch: batch_tokenize_preprocess_decoder(\n",
    "        batch, tokenizer, max_length\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n",
    "\n",
    "val_ds = val_ds.map(\n",
    "    lambda batch: batch_tokenize_preprocess_decoder(\n",
    "        batch, tokenizer, max_length\n",
    "    ),\n",
    "    batched=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mid4thomas\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/yrsong/personal/prefixtuning/train_chatbot/wandb/run-20221203_001521-g0f1fve6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/id4thomas/prefixtuning-chatbot/runs/g0f1fve6\" target=\"_blank\">pretty-frog-4</a></strong> to <a href=\"https://wandb.ai/id4thomas/prefixtuning-chatbot\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PREPARE TRAIN\n",
    "wandb.init(project=\"prefixtuning-chatbot\", entity = \"id4thomas\")\n",
    "wandb.watch(model, log=\"all\", log_freq=10)\n",
    "\n",
    "out_dir = \"weights\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "        run_name = \"prefix_gptneox_chatbot\",\n",
    "\n",
    "        # Train Params\n",
    "        ## Steps/Epochs\n",
    "        num_train_epochs = 3,\n",
    "\n",
    "        ## LR\n",
    "        learning_rate = 5e-5,\n",
    "        ## Batch\n",
    "        per_device_train_batch_size = 32,\n",
    "        per_device_eval_batch_size = 32,\n",
    "        gradient_accumulation_steps = 1,\n",
    "        ## ETC\n",
    "        # label_smoothing_factor = config[\"label_smoothing_factor\"],\n",
    "\n",
    "        # Checkpointing, Saving\n",
    "        output_dir = os.path.join(out_dir,\"checkpoints\"),\n",
    "        save_strategy = \"steps\", # steps, epoch\n",
    "        save_steps = 80,\n",
    "        save_total_limit = 1,\n",
    "        load_best_model_at_end = True,\n",
    "        overwrite_output_dir=True,\n",
    "\n",
    "        # Evaluating\n",
    "        evaluation_strategy = \"steps\",\n",
    "        metric_for_best_model = \"eval_loss\",\n",
    "\n",
    "        # Logging\n",
    "        logging_dir = out_dir,\n",
    "        logging_steps = 80,\n",
    "        disable_tqdm = False,\n",
    "        report_to = \"wandb\",\n",
    "        # predict_with_generate = True,\n",
    "\n",
    "        # System\n",
    "        seed = random_seed,\n",
    "        fp16 = False,\n",
    "        bf16 = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "        model = model,\n",
    "        args = training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset = tr_ds,\n",
    "        eval_dataset = val_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "/home/yrsong/anaconda3/envs/huggingface/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 9576\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 900\n",
      "  Number of trainable parameters = 80397088\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 23:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.236100</td>\n",
       "      <td>1.870337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.834800</td>\n",
       "      <td>1.826008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.798000</td>\n",
       "      <td>1.803525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>1.788400</td>\n",
       "      <td>1.791224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.740500</td>\n",
       "      <td>1.778697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.735800</td>\n",
       "      <td>1.770133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>1.731200</td>\n",
       "      <td>1.759744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>1.698800</td>\n",
       "      <td>1.754155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.687200</td>\n",
       "      <td>1.750580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.683700</td>\n",
       "      <td>1.746201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>1.691100</td>\n",
       "      <td>1.744106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-80\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-160\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-80] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-240\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-160] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-320\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-240] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-400\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-320] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-480\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-400] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-560\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-480] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-640\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-560] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-720\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-640] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-800\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-720] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `PrefixGPTNeoXLMHeadModel.forward` and have been ignored: source, target. If source, target are not expected by `PrefixGPTNeoXLMHeadModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1064\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to weights/checkpoints/checkpoint-880\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "Deleting older checkpoint [weights/checkpoints/checkpoint-800] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from weights/checkpoints/checkpoint-880 (score: 1.7441061735153198).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=1.7819067086113825, metrics={'train_runtime': 1413.535, 'train_samples_per_second': 20.324, 'train_steps_per_second': 0.637, 'total_flos': 0.0, 'train_loss': 1.7819067086113825, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['input_tokens', 'wte.weight', 'control_trans.0.weight', 'control_trans.0.bias', 'control_trans.2.weight', 'control_trans.2.bias']\n"
     ]
    }
   ],
   "source": [
    "# Only save prefix weights\n",
    "state_dict = model.state_dict()\n",
    "layer_keys = list(state_dict.keys())\n",
    "\n",
    "filtered = list(filter(lambda x: \"pretrain_model\" not in x, layer_keys))\n",
    "print(filtered)\n",
    "\n",
    "processed_dict = {}\n",
    "for k in filtered:\n",
    "    processed_dict[k] = state_dict[k]\n",
    "torch.save(processed_dict, os.path.join(\"prefix_weights\", \"gptneox_ep30_1r1e-5.bin\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to weights/best\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "tokenizer config file saved in weights/best/tokenizer_config.json\n",
      "Special tokens file saved in weights/best/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('weights/best/tokenizer_config.json',\n",
       " 'weights/best/special_tokens_map.json',\n",
       " 'weights/best/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(os.path.join(\"weights\",\"best\"))\n",
    "tokenizer.save_pretrained(os.path.join(\"weights\",\"best\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface",
   "language": "python",
   "name": "huggingface"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
